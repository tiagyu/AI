{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a29d61e",
   "metadata": {},
   "source": [
    "# 예제 2.1 토큰화 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92cd93a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_text_list : ['나는', '최근', '파리', '여행을', '다녀왔다']\n"
     ]
    }
   ],
   "source": [
    "# 띄어쓰기 단위로 분리\n",
    "input_text = \"나는 최근 파리 여행을 다녀왔다\"\n",
    "input_text_list = input_text.split()\n",
    "print(f\"input_text_list : {input_text_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb5d6b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "str2idx : {'나는': 0, '최근': 1, '파리': 2, '여행을': 3, '다녀왔다': 4}\n",
      "idx2str : {0: '나는', 1: '최근', 2: '파리', 3: '여행을', 4: '다녀왔다'}\n"
     ]
    }
   ],
   "source": [
    "# 토큰 -> 아이디 딕셔너리와 아이디 -> 토큰 딕셔너리 만들기\n",
    "str2idx = {word : idx for idx, word in enumerate(input_text_list)}\n",
    "idx2str = {idx : word for idx, word in enumerate(input_text_list)}\n",
    "print(f\"str2idx : {str2idx}\")\n",
    "print(f\"idx2str : {idx2str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "820868d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# 토큰을 토큰 아이디로 변환\n",
    "input_ids = [str2idx[word] for word in input_text_list]\n",
    "print(f\"input_ids : {input_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d82fdf",
   "metadata": {},
   "source": [
    "# 예제 2.2 토큰 아이디에서 벡터로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83819fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "embedding_dim = 16\n",
    "embed_layer = nn.Embedding(len(str2idx), embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc431609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 16])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings = embed_layer(torch.tensor(input_ids))\n",
    "input_embeddings = input_embeddings.unsqueeze(0)\n",
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9412d18c",
   "metadata": {},
   "source": [
    "# 예제 2.3 절대적 위치 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdc49f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 16])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "embedding_dim = 16\n",
    "max_position = 12\n",
    "position_embed_layer = nn.Embedding(len(str2idx), embedding_dim)\n",
    "position_embed_layer = nn.Embedding(max_position, embedding_dim)\n",
    "\n",
    "position_ids = torch.arange(len(input_ids), dtype=torch.long).unsqueeze(0)\n",
    "position_encodings = position_embed_layer(position_ids)\n",
    "token_embeddings = embed_layer(torch.tensor(input_ids)) # (5,16)\n",
    "token_embeddings = token_embeddings.unsqueeze(0) # (1,5,16)\n",
    "input_embeddings = token_embeddings + position_encodings\n",
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8175c097",
   "metadata": {},
   "source": [
    "# 2.4 쿼리, 키, 값 벡터를 만드는 nn.linear 층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95744186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "querys : tensor([[[-1.7437, -0.3612,  1.1627,  0.0903,  1.5031,  0.4264, -0.7657,\n",
      "          -0.4169, -0.5126, -0.8615,  0.0235, -1.0073,  0.3954, -0.8625,\n",
      "           0.6526,  0.9804],\n",
      "         [ 0.4367,  0.6721,  0.6141,  2.1430,  0.7625,  1.7115, -0.4548,\n",
      "           2.6210, -0.4420,  0.5068, -1.4000,  2.2586, -0.4002,  1.2673,\n",
      "          -0.1687,  0.6671],\n",
      "         [ 0.9893,  0.7571, -0.9151,  0.2145, -0.9631, -0.9537, -0.1105,\n",
      "          -0.6878, -0.1830,  1.0054, -0.1079,  1.2545,  0.5934, -0.6866,\n",
      "           0.1977, -0.5988],\n",
      "         [ 1.1696, -0.1538,  1.5511, -1.3463,  0.2340,  0.3890,  0.9434,\n",
      "          -1.0653,  1.2194, -0.0922,  1.7279, -0.0535,  1.0757,  0.5089,\n",
      "           0.2536,  0.4214],\n",
      "         [-0.4564, -0.7045, -0.7121,  1.1968, -1.0562, -1.0733,  0.8456,\n",
      "           0.5988, -1.3383,  0.6284, -0.4088, -0.2570,  0.0872, -0.0624,\n",
      "          -0.3653,  0.3548]]], grad_fn=<ViewBackward0>)\n",
      "keys : tensor([[[-0.7745, -1.2311,  1.0522,  1.7446,  0.7581,  0.7985,  0.3313,\n",
      "           0.2107,  0.5096,  0.8182, -0.9112,  0.7558,  1.8569,  0.5118,\n",
      "          -0.3754, -0.0196],\n",
      "         [-1.6367, -1.0965, -0.8654, -1.8663,  1.0559, -0.5208, -0.2770,\n",
      "          -0.9866,  0.4841,  0.1376, -0.0046,  0.1001, -0.2470,  1.1616,\n",
      "          -1.2136,  1.8694],\n",
      "         [ 0.5012,  0.7249, -0.4064,  0.3116, -0.2203,  0.9420,  0.0801,\n",
      "           1.1373,  0.2179, -1.2236, -0.4484, -0.7354, -0.1329,  0.9396,\n",
      "          -0.0907, -0.2987],\n",
      "         [ 0.5854,  0.9563, -0.3337,  0.4269,  0.0345, -0.8332,  0.3994,\n",
      "           0.7545, -1.1646,  0.0199,  0.9134,  0.0279,  0.5225, -0.4674,\n",
      "          -1.3238, -0.1730],\n",
      "         [-0.1263,  0.5180,  0.4448, -0.5486, -1.0922, -1.3575, -0.0489,\n",
      "          -0.4205,  0.5837, -1.0683,  0.9254, -1.1886, -1.7197,  0.7890,\n",
      "           0.0320,  0.0691]]], grad_fn=<ViewBackward0>)\n",
      "values : tensor([[[ 0.8153,  0.6335, -0.2753,  0.6660,  0.0702,  1.2804,  0.0956,\n",
      "          -0.3636, -1.6332, -0.6680, -0.0966, -0.0129, -1.5076,  0.7260,\n",
      "          -0.1344, -0.8639],\n",
      "         [ 0.8138, -2.1198,  0.6163,  0.0355,  0.2151, -1.2928,  0.3119,\n",
      "           1.1261, -0.1034,  1.3067, -0.5003,  0.1253,  0.8971,  1.1837,\n",
      "          -0.6159, -0.5162],\n",
      "         [-0.3809,  1.7133,  1.2680, -0.8237, -1.6363,  0.2883, -0.3557,\n",
      "           1.6982, -0.3265, -0.0725, -0.2223, -0.1156, -0.1002,  0.5284,\n",
      "           1.2835,  0.0839],\n",
      "         [-0.8210,  0.2133, -0.1176, -0.0605, -0.1031, -1.8054, -0.6234,\n",
      "          -0.3391,  2.2828, -1.4199,  0.1084, -0.4318,  0.3330, -0.8118,\n",
      "          -0.1058, -1.0872],\n",
      "         [-1.3863, -1.2461,  1.2173, -0.7644,  1.4862,  0.9261, -0.1136,\n",
      "           0.1287, -0.2419,  0.5932,  1.2608,  0.1467, -0.9729, -0.6660,\n",
      "           0.4918,  2.2538]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "head_dim = 16\n",
    "\n",
    "# 쿼리, 키, 값을 계산하기 위한 변환\n",
    "weight_q = nn.Linear(embedding_dim, head_dim)\n",
    "weight_k = nn.Linear(embedding_dim, head_dim)\n",
    "weight_v = nn.Linear(embedding_dim, head_dim)\n",
    "\n",
    "# 변환 수행\n",
    "# (1,5,16)\n",
    "querys = weight_q(input_embeddings)\n",
    "print(f\"querys : {querys}\")\n",
    "\n",
    "# (1,5,16)\n",
    "keys = weight_k(input_embeddings)\n",
    "print(f\"keys : {keys}\")\n",
    "\n",
    "# (1,5,16)\n",
    "values = weight_v(input_embeddings)\n",
    "print(f\"values : {values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab2e48f",
   "metadata": {},
   "source": [
    "# 2.5 스케일 점곱 방식의 어텐션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b387d715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_attention(querys, keys, values, is_causal = False):\n",
    "    dim_k = querys.size(-1) # 16\n",
    "    # 1. 쿼리와 키를 곱한다. 분산이 커지는 것을 방지하기 위해 임베딩 차원 수의 제곱근으로 나눈다\n",
    "    scores = querys @ keys.transpose(-2, -1) / sqrt(dim_k)\n",
    "    \n",
    "    # 2. 쿼리와 키를 곱해 게산한 스코어(scores)를 합이 1이 되도록 소프트맥스를 취해 가중치로 바꾼다\n",
    "    weights = F.softmax(scores, dim=1)\n",
    "    \n",
    "    # 3. 가중치와 값을 곱해 입력과 동일한 형태의 출력을 반환한다\n",
    "    return weights @ values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce358c4",
   "metadata": {},
   "source": [
    "# 2.6 어텐션 연산의 입력과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4133d5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 입력 형태 : torch.Size([1, 5, 16])\n",
      "어텐션 적용 후 형태 : torch.Size([1, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "print(f\"원본 입력 형태 : {input_embeddings.shape}\")\n",
    "\n",
    "after_attention_embeddings = compute_attention(querys, keys, values)\n",
    "\n",
    "print(f\"어텐션 적용 후 형태 : {after_attention_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629473eb",
   "metadata": {},
   "source": [
    "# 2.7 어텐션 연산의 입력과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8709c21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, token_embed_dim, head_dim, is_causal=False):\n",
    "        super().__init__()\n",
    "        self.is_causal = is_causal\n",
    "        # 쿼리 벡터 생성을 위한 선형 층\n",
    "        self.weight_q = nn.Linear(token_embed_dim, head_dim)\n",
    "        \n",
    "        # 키 벡터 생성을 위한 선형 층\n",
    "        self.weight_k = nn.Linear(token_embed_dim, head_dim)\n",
    "        \n",
    "        # 값 벡터 생성을 위한 선형 층\n",
    "        self.weight_v = nn.Linear(token_embed_dim, head_dim)\n",
    "        \n",
    "    def forward(self, querys, keys, values):\n",
    "        outputs = compute_attention(\n",
    "            # 쿼리 벡터\n",
    "            self.weight_q(querys),\n",
    "            \n",
    "            # 키 벡터\n",
    "            self.weight_k(keys),\n",
    "            \n",
    "            # 값 벡터\n",
    "            self.weight_v(values),\n",
    "            is_causal = self.is_causal\n",
    "        )\n",
    "    \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb490b60",
   "metadata": {},
   "source": [
    "# 2.8 멀티 헤드 어텐션 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ee23e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 16])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, token_embed_dim, d_model, n_head, is_causal=False):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.is_causal = is_causal\n",
    "        # 쿼리 벡터 생성을 위한 선형 층\n",
    "        self.weight_q = nn.Linear(token_embed_dim, head_dim)\n",
    "        \n",
    "        # 키 벡터 생성을 위한 선형 층\n",
    "        self.weight_k = nn.Linear(token_embed_dim, head_dim)\n",
    "        \n",
    "        # 값 벡터 생성을 위한 선형 층\n",
    "        self.weight_v = nn.Linear(token_embed_dim, head_dim)\n",
    "        \n",
    "        self.concat_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, querys, keys, values):\n",
    "        B, T, C = querys.size()\n",
    "        querys = self.weight_q(querys).view(B, T, self.n_head, C // self.n_head).transpose(1,2)\n",
    "        keys = self.weight_k(keys).view(B, T, self.n_head, C // self.n_head).transpose(1,2)\n",
    "        values = self.weight_v(values).view(B, T, self.n_head, C // self.n_head).transpose(1,2)\n",
    "        attention = compute_attention(querys, keys, values, self.is_causal)\n",
    "        output = attention.transpose(1,2).contiguous().view(B, T, C)\n",
    "        output = self.concat_linear(output)\n",
    "        return output\n",
    "        \n",
    "n_head = 4\n",
    "mh_attention = MultiheadAttention(embedding_dim, embedding_dim, n_head)\n",
    "after_attention_embeddings = mh_attention(input_embeddings, input_embeddings, input_embeddings)\n",
    "after_attention_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a766aa8b",
   "metadata": {},
   "source": [
    "# 2.9 총 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "875b3caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.2042,  0.2211, -0.1273,  0.4861, -0.2177, -0.1498, -0.2115,  0.3610,\n",
       "          -0.1572,  0.1849, -0.3571, -0.0033,  0.1521, -0.3891,  0.0834, -0.0796]]),\n",
       " tensor([[0.9781, 1.1271, 1.0244, 1.4549, 1.0181, 0.8899, 0.9389, 1.1653, 0.7113,\n",
       "          0.9844, 1.1472, 0.9055, 0.7757, 1.3902, 1.2438, 1.2820]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = nn.LayerNorm(embedding_dim)\n",
    "norm_x = norm(input_embeddings)\n",
    "print(norm_x.shape)\n",
    "\n",
    "# 실제로 평균과 표준편차 확인하기\n",
    "norm_x.mean(dim=1).data, norm_x.std(dim=1).data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8127d2",
   "metadata": {},
   "source": [
    "# 2.10 피드 포워드 층 코드드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870fd47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreLayerNormFeedForward(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_application_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
