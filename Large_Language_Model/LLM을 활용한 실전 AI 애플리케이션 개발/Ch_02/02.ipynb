{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a29d61e",
   "metadata": {},
   "source": [
    "# 예제 2.1 토큰화 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92cd93a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_text_list : ['나는', '최근', '파리', '여행을', '다녀왔다']\n"
     ]
    }
   ],
   "source": [
    "# 띄어쓰기 단위로 분리\n",
    "input_text = \"나는 최근 파리 여행을 다녀왔다\"\n",
    "input_text_list = input_text.split()\n",
    "print(f\"input_text_list : {input_text_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb5d6b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "str2idx : {'나는': 0, '최근': 1, '파리': 2, '여행을': 3, '다녀왔다': 4}\n",
      "idx2str : {0: '나는', 1: '최근', 2: '파리', 3: '여행을', 4: '다녀왔다'}\n"
     ]
    }
   ],
   "source": [
    "# 토큰 -> 아이디 딕셔너리와 아이디 -> 토큰 딕셔너리 만들기\n",
    "str2idx = {word : idx for idx, word in enumerate(input_text_list)}\n",
    "idx2str = {idx : word for idx, word in enumerate(input_text_list)}\n",
    "print(f\"str2idx : {str2idx}\")\n",
    "print(f\"idx2str : {idx2str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "820868d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# 토큰을 토큰 아이디로 변환\n",
    "input_ids = [str2idx[word] for word in input_text_list]\n",
    "print(f\"input_ids : {input_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d82fdf",
   "metadata": {},
   "source": [
    "# 예제 2.2 토큰 아이디에서 벡터로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83819fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "embedding_dim = 16\n",
    "embed_layer = nn.Embedding(len(str2idx), embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc431609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 16])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings = embed_layer(torch.tensor(input_ids))\n",
    "input_embeddings = input_embeddings.unsqueeze(0)\n",
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9412d18c",
   "metadata": {},
   "source": [
    "# 예제 2.3 절대적 위치 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdc49f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 16])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "embedding_dim = 16\n",
    "max_position = 12\n",
    "position_embed_layer = nn.Embedding(len(str2idx), embedding_dim)\n",
    "position_embed_layer = nn.Embedding(max_position, embedding_dim)\n",
    "\n",
    "position_ids = torch.arange(len(input_ids), dtype=torch.long).unsqueeze(0)\n",
    "position_encodings = position_embed_layer(position_ids)\n",
    "token_embeddings = embed_layer(torch.tensor(input_ids)) # (5,16)\n",
    "token_embeddings = token_embeddings.unsqueeze(0) # (1,5,16)\n",
    "input_embeddings = token_embeddings + position_encodings\n",
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8175c097",
   "metadata": {},
   "source": [
    "# 2.4 쿼리, 키, 값 벡터를 만드는 nn.linear 층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95744186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "querys : tensor([[[-0.2961,  0.6854,  0.1530, -0.3426, -0.0741,  0.3178,  0.8699,\n",
      "          -0.3985, -0.5971, -0.6199, -0.5187, -0.6099,  0.0852, -0.2142,\n",
      "          -0.0194,  0.4858],\n",
      "         [ 0.4327,  1.1523, -0.8210, -0.5073,  0.9834, -0.1231,  0.0214,\n",
      "          -0.0982, -0.3537, -0.5162,  1.0381, -0.9395, -1.3320,  0.8599,\n",
      "           0.0905, -0.0640],\n",
      "         [ 0.3872, -0.0376, -0.3188,  0.5390,  0.0762, -0.2741, -0.0638,\n",
      "           0.1104,  0.5756, -0.1870,  1.6786, -1.5408,  0.1511, -0.9097,\n",
      "          -0.9001, -1.6500],\n",
      "         [-0.4406,  0.3879,  0.8989,  0.5641, -1.0762,  0.8097, -0.0108,\n",
      "          -0.9281, -1.0690,  0.9965, -1.9264,  0.8267,  0.6689, -0.5009,\n",
      "           0.1020,  0.0233],\n",
      "         [-0.0690, -0.1553, -1.3841, -0.1809,  0.7826, -0.5997, -0.5168,\n",
      "          -0.8792,  0.4327, -0.6208,  0.6876, -0.3782, -0.6043,  0.5955,\n",
      "          -0.0898, -0.0042]]], grad_fn=<ViewBackward0>)\n",
      "keys : tensor([[[ 0.5223,  0.1812,  1.3983, -0.6483,  0.8094, -0.6200, -1.0524,\n",
      "           1.2878, -0.4391,  0.4728,  0.7610,  0.1151, -0.5833,  0.0682,\n",
      "          -0.3411,  0.4636],\n",
      "         [ 0.4689, -1.2607,  1.0136, -0.3684, -1.1939,  0.5014,  0.1435,\n",
      "          -0.1064, -1.1067,  0.6973,  1.8788, -0.1219, -0.3373,  0.1954,\n",
      "          -0.2971, -0.6710],\n",
      "         [ 1.0777,  0.7138,  0.2779,  0.9094,  0.0487, -1.0964, -1.0877,\n",
      "           1.4169, -0.6245, -1.1662,  0.7325, -0.4555, -1.5161,  0.0691,\n",
      "          -1.0206,  0.5377],\n",
      "         [ 0.5996,  1.6283, -1.2041,  1.5792,  0.4999, -0.4563,  0.7990,\n",
      "          -1.4555,  0.0801, -0.0153,  0.6460,  0.0735, -0.2945, -1.2853,\n",
      "           0.6005, -0.6253],\n",
      "         [-0.3235, -0.9852,  0.0279, -1.1876,  0.6043,  0.5207, -0.1311,\n",
      "           0.7902, -0.3740, -0.1797, -0.3614,  0.5011,  0.1869,  0.3050,\n",
      "          -0.7027,  0.0484]]], grad_fn=<ViewBackward0>)\n",
      "values : tensor([[[-0.4564, -0.2612, -0.7016, -0.3176, -0.7394, -1.1171,  1.0220,\n",
      "          -0.7297, -0.5601, -0.0620,  0.3906, -0.3098, -0.3349, -0.7742,\n",
      "           0.7661,  0.1145],\n",
      "         [ 0.0045, -0.4125, -0.4842, -0.9012, -2.0581, -0.9132,  1.9019,\n",
      "           0.7282, -0.9374, -1.1733,  1.0870, -1.0532,  1.4670,  0.0175,\n",
      "          -1.2297, -0.9512],\n",
      "         [-1.2394,  1.1241, -0.2238, -0.0285, -1.2064,  0.7458,  0.1525,\n",
      "           0.5104, -0.8888,  0.6832, -0.6661, -0.1030,  0.4542, -0.1216,\n",
      "           1.4817,  1.6925],\n",
      "         [ 0.4672,  0.0319,  0.0523,  1.7815,  0.1140, -0.0541, -0.7053,\n",
      "           0.2877,  1.1195, -0.9335,  0.1631, -0.2880, -0.1414,  0.0906,\n",
      "          -0.2191, -0.0905],\n",
      "         [-0.1941,  0.1930,  0.1852, -1.3583, -0.1612,  0.9445, -0.5056,\n",
      "          -0.3115, -0.9208,  0.2144,  0.1462,  0.6812, -0.9422,  0.3145,\n",
      "          -0.1687,  0.8928]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "head_dim = 16\n",
    "\n",
    "# 쿼리, 키, 값을 계산하기 위한 변환\n",
    "weight_q = nn.Linear(embedding_dim, head_dim)\n",
    "weight_k = nn.Linear(embedding_dim, head_dim)\n",
    "weight_v = nn.Linear(embedding_dim, head_dim)\n",
    "\n",
    "# 변환 수행\n",
    "# (1,5,16)\n",
    "querys = weight_q(input_embeddings)\n",
    "print(f\"querys : {querys}\")\n",
    "\n",
    "# (1,5,16)\n",
    "keys = weight_k(input_embeddings)\n",
    "print(f\"keys : {keys}\")\n",
    "\n",
    "# (1,5,16)\n",
    "values = weight_v(input_embeddings)\n",
    "print(f\"values : {values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab2e48f",
   "metadata": {},
   "source": [
    "# 2.5 스케일 점곱 방식의 어텐션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b387d715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_attention(querys, keys, values, is_causal = False):\n",
    "    dim_k = querys.size(-1) # 16\n",
    "    # 1. 쿼리와 키를 곱한다. 분산이 커지는 것을 방지하기 위해 임베딩 차원 수의 제곱근으로 나눈다\n",
    "    scores = querys @ keys.transpose(-2, -1) / sqrt(dim_k)\n",
    "    \n",
    "    # 2. 쿼리와 키를 곱해 게산한 스코어(scores)를 합이 1이 되도록 소프트맥스를 취해 가중치로 바꾼다\n",
    "    weights = F.softmax(scores, dim=1)\n",
    "    \n",
    "    # 3. 가중치와 값을 곱해 입력과 동일한 형태의 출력을 반환한다\n",
    "    return weights @ values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce358c4",
   "metadata": {},
   "source": [
    "# 2.6 어텐션 연산의 입력과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4133d5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 입력 형태 : torch.Size([1, 5, 16])\n",
      "어텐션 적용 후 형태 : torch.Size([1, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "print(f\"원본 입력 형태 : {input_embeddings.shape}\")\n",
    "\n",
    "after_attention_embeddings = compute_attention(querys, keys, values)\n",
    "\n",
    "print(f\"어텐션 적용 후 형태 : {after_attention_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629473eb",
   "metadata": {},
   "source": [
    "# 2.7 어텐션 연산의 입력과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8709c21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, token_embed_dim, head_dim, is_causal=False):\n",
    "        super().__init__()\n",
    "        self.is_causal = is_causal\n",
    "        # 쿼리 벡터 생성을 위한 선형 층\n",
    "        self.weight_q = nn.Linear(token_embed_dim, head_dim)\n",
    "        \n",
    "        # 키 벡터 생성을 위한 선형 층\n",
    "        self.weight_k = nn.Linear(token_embed_dim, head_dim)\n",
    "        \n",
    "        # 값 벡터 생성을 위한 선형 층\n",
    "        self.weight_v = nn.Linear(token_embed_dim, head_dim)\n",
    "        \n",
    "    def forward(self, querys, keys, values):\n",
    "        outputs = compute_attention(\n",
    "            # 쿼리 벡터\n",
    "            self.weight_q(querys),\n",
    "            \n",
    "            # 키 벡터\n",
    "            self.weight_k(keys),\n",
    "            \n",
    "            # 값 벡터\n",
    "            self.weight_v(values),\n",
    "            is_causal = self.is_causal\n",
    "        )\n",
    "    \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb490b60",
   "metadata": {},
   "source": [
    "# 2.8 멀티 헤드 어텐션 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee23e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, token_embed_dim, d_model, n_head, is_causal=False):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.is_causal = is_causal\n",
    "        # 쿼리 벡터 생성을 위한 선형 층\n",
    "        self.weight_q = nn.Linear(token_embed_dim, head_dim)\n",
    "        \n",
    "        # 키 벡터 생성을 위한 선형 층\n",
    "        self.weight_k = nn.Linear(token_embed_dim, head_dim)\n",
    "        \n",
    "        # 값 벡터 생성을 위한 선형 층\n",
    "        self.weight_v = nn.Linear(token_embed_dim, head_dim)\n",
    "        \n",
    "        self.concat_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, querys, keys, values):\n",
    "        outputs = compute_attention(\n",
    "            # 쿼리 벡터\n",
    "            self.weight_q(querys),\n",
    "            \n",
    "            # 키 벡터\n",
    "            self.weight_k(keys),\n",
    "            \n",
    "            # 값 벡터\n",
    "            self.weight_v(values),\n",
    "            is_causal = self.is_causal\n",
    "        )\n",
    "    \n",
    "        return outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_application_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
